---
title: Hamiltonian Descent Methods を chainer で実装した
tags: 機械学習 Python Chainer
author: omi_UT
slide: false
---
本記事では Hamiltonian Descent Methods を実装して動かすところまでをします。
[GitHub レポジトリ](https://github.com/omi-key/Hamiltonian-Descent-Methods/)

[続きました](https://qiita.com/omi_UT/items/07e366e56b20dbfd50de)

# Hamiltonian Descent Methods (HDMs) とは
2018 年 9 月に Deepmind により発表された最適化手法です。[原論文](https://arxiv.org/pdf/1809.05042.pdf)

内容は相当難しく、自分もかなりの部分を読み飛ばしています。日本語で解説されている記事として @ZoneTsuyoshi さんの [Hamiltonian Descent Methods ~より広範なクラスで1次収束を達成する最適化手法~](https://qiita.com/ZoneTsuyoshi/items/37809abfdce747ecd6d5) がありますので、気になる方はそちらを一読してきてからのほうが良いと思います。

数学的なありがたみを全部投げ捨てると、大事な部分は以下の 3 点になります。

* $x$ のほかに運動量 $p$ を更新する必要がある。
* First Explicit Method が次式で表される。

```math
p_{i+1}=\delta p_i-\epsilon\delta\nabla f(x_i)\\
x_{i+1}=x_i+\epsilon\nabla k(p_{i+1})
```

* 運動エネルギー $k$ を $k(p)=\varphi_2^1(\\|p\\|)=\sqrt{\\|p\\|^2+1}-1$ にするとなんとかなることがある。[^1]

実際にはこれが成立する数学的条件を考えていかないといけないのですが、とりあえずこれで実装します。

# Chainer を使って実装
Chainer 公式の [optimizer ディレクトリ](https://github.com/chainer/chainer/tree/master/chainer/optimizers) 以下の SGD とか Adam とかの中身をパクりながら[実装しました](https://github.com/omi-key/Hamiltonian-Descent-Methods/blob/master/HDMs.ipynb)。[^2] 実装に当たっては、山たー・優曇華院 さんによる [Hamiltonian Descent Methodsの実装についての解説](https://omedstu.jimdo.com/2018/09/26/hamiltonian-descent-methods%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E8%A7%A3%E8%AA%AC/) 上で紹介されている [実装](https://github.com/takyamamoto/FirstExplicitMethod-HDM/blob/master/first_explicit_method_chainer.py) を参考にしています。
[^2]: [4] 番のブロック。 上手いことリンク張れるのかもしれませんが jupyter notebook の使い方わからないので… <br> ついでに吐かせたログを消す方法も知らず
<br>
大事なのはここ。上の式をそのまま python で書いているだけです。
GPU 周りが全くわからなかったので CPU 版と同一ですが一応動きます…

```python
    def update_core_gpu(self, param):
        grad = param.grad
        if grad is None:
            return
        hp = self.hyperparam
        p = self.state['p']
        
        p_ip1 = hp.delta * p - hp.epsilon * hp.delta * grad
        
        p_ip1var = Variable(p_ip1)

        sqsum = chainer.functions.sum(p_ip1var ** 2.0)
        kinetic = (1 + sqsum ) ** 0.5-1
        
        kinetic.backward()
        grad_k = p_ip1var.grad
        
        p = p_ip1
        param.data += hp.epsilon * grad_k
```

# 実験
MNIST を 500 次元の中間層 2 つ経由して 10 次元にします。途中で適宜 Relu を掛けます。training は 100 サンプルずつのバッチを使い 100 epoch。

optimizer だけを変更し、上の通りの HDM First Explicit Method と Adam のデフォルトとで比較します。
HDM のハイパーパラメータは $\epsilon = 1.0, \gamma = 2/3 (,\delta = (1+\gamma\epsilon)^{-1} = 0.6)$ です。 [^3] 

今回 optimizer の質が見たいので main/loss だけを見ます。validation とか accuracy とかは無視で

# 結果
画像は全部 jupyter notebook の下の方に貼ってあるのを引っ張ってきてます。

片対数グラフで見てもちゃんと loss が減少している
<img src="https://qiita-image-store.s3.amazonaws.com/0/266393/6cce0f50-43b5-aaa6-b0f5-3024f5e24f4a.png" width=60%>
ただし速度は遅い。
<img src="https://qiita-image-store.s3.amazonaws.com/0/266393/5f8c2168-ee38-e8e9-9168-0d89e3ef0b52.png" width=60%>

#まとめ
* たぶん Chainer で動く HDM を実装した
* epoch 数で見ると収束が速い
    * 実装がまともなら実行時間で見てもよくなるのだろうか？　→[なりました](https://qiita.com/omi_UT/items/07e366e56b20dbfd50de)
    * MNIST だと loss が減るのが早すぎる気がした


[^3]: $\epsilon$ が狂気の沙汰ですが $f$ ではなく $\nabla k$ にかかっている式の意味を考えたら 1 でもいいかなあと。$\gamma$ は適当。これでうまくいったので、よしとします。



[^1]: [Hamiltonian Descent Methods ~より広範なクラスで1次収束を達成する最適化手法~](https://qiita.com/ZoneTsuyoshi/items/37809abfdce747ecd6d5) <br><br>  **relativistic kinetic** と呼ばれ，微分すると， $$\nabla k(p)=\frac{p}{\sqrt{\\|p\\|^2+1}}$$ となり，様々な適応的勾配降下法で使われている形になる．

